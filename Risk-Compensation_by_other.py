# -*- coding: utf-8 -*-
"""진짜최종.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AG1Ip6S7gDOmVByqpzaUDZkU8W9kzlvT
"""

import shutil
from google.colab import files

# 폴더를 압축하여 .zip 파일 생성
shutil.make_archive('/content/기존100_coco', 'zip', '/content/runs/detect/train4')
# shutil.make_archive('/content/원본 시각화', 'zip', '/content')

# 생성된 .zip 파일 다운로드
files.download('/content/기존100_coco.zip')

!pip install ultralytics
!pip install torch
!pip install tensorboard matplotlib

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir=runs/

import matplotlib.pyplot as plt
import matplotlib.patches as patches
import math

def visualize_layer_risk(layer_risk_val_dict, max_layers_per_chart=200, label_skip=1):
    """
    시각화 함수: 단일 데이터셋인 layer_risk_val_dict를 그래프로 시각화합니다.
    데이터 길이에 따라 자동으로 그래프 크기와 개수를 조정하며,
    x축 레이블을 적당히 생략하고, 큰 박스를 추가하여 데이터를 구분합니다.

    Parameters:
        layer_risk_val_dict (dict): 레이어 인덱스를 키로, 위험 값을 값으로 가지는 딕셔너리.
        max_layers_per_chart (int): 한 그래프에 표시할 최대 레이어 수. 기본값은 200.
        label_skip (int): x축 레이블 표시 간격. 기본값은 1 (모든 레이블 표시).
    """
    if not isinstance(layer_risk_val_dict, dict) or not layer_risk_val_dict:
        print("Input must be a non-empty dictionary.")
        return

    # 레이어 이름과 값 분리
    layers = list(layer_risk_val_dict.keys())
    risks = list(layer_risk_val_dict.values())

    # 전체 데이터 분할
    total_layers = len(layers)
    num_charts = math.ceil(total_layers / max_layers_per_chart)  # 필요한 그래프 수 계산

    fig, axes = plt.subplots(num_charts, 1, figsize=(12, 6 * num_charts))  # 세로로 여러 그래프
    if num_charts == 1:
        axes = [axes]  # 단일 그래프일 경우에도 리스트로 처리

    for i in range(num_charts):
        # 현재 차트에 표시할 데이터 슬라이스
        start_idx = i * max_layers_per_chart
        end_idx = start_idx + max_layers_per_chart
        current_layers = layers[start_idx:end_idx]
        current_risks = risks[start_idx:end_idx]

        # 레이블 생략 간격 계산
        skip_interval = max(1, math.ceil(len(current_layers) / (12 / label_skip)))
        adjusted_labels = [
            label if idx % skip_interval == 0 else ""
            for idx, label in enumerate(current_layers)
        ]

        # 그래프 그리기
        ax = axes[i]
        ax.bar(current_layers, current_risks, color='skyblue', edgecolor='black')
        ax.set_xticks(range(len(current_layers)))
        ax.set_xticklabels(adjusted_labels, rotation=45, fontsize=10, ha='right')
        ax.set_xlabel("Layer Indices", fontsize=12)
        ax.set_ylabel("Risk Values", fontsize=12)
        ax.set_title(f"Layer Risk Values (Chart {i + 1}/{num_charts})", fontsize=14)
        ax.grid(axis='y', linestyle='--', alpha=0.7)

        # 큰 박스 추가
        box = patches.Rectangle(
            (0, min(current_risks) * 0.95),  # 좌측 하단 좌표
            len(current_layers) - 1,        # 너비
            max(current_risks) * 1.05 - min(current_risks) * 0.95,  # 높이
            edgecolor='red',
            facecolor='none',
            linewidth=2,
            linestyle='--'
        )
        ax.add_patch(box)

    plt.tight_layout()
    plt.show()






# 사용 예시
visualize_layer_risk(model.layer_risk_val_pos)
# 사용 예시
visualize_layer_risk(model.layer_risk_val)
visualize_layer_risk(model.modified_data_dict)
visualize_layer_risk(model.layer_energies)

"""##코드"""

import torch
import torch.nn as nn
from ultralytics import YOLO
import numpy as np
import pandas as pd
from collections import Counter
import math
from scipy.optimize import minimize_scalar
import matplotlib.pyplot as plt
import matplotlib.pyplot as plt
from torch.utils.tensorboard import SummaryWriter
import numpy as np
import sys
import os
from contextlib import contextmanager
import math
from scipy.optimize import fsolve
class RISK_FULL(YOLO):
    def __init__(self, model_path):
        super().__init__(model_path)
        self.layer_energies = {}
        self.energy_ratios = {}
        self.closest_log_bases = {}
        self.debug_logs = []
        self.modelid = model_path
        self.call_count = 0  # 호출 횟수를 저장할 변수
        self.layer_risk_val = {}
        self.layer_risk_val_pos = {}
        self.achieved_cvs = {}
        self.modified_data_dict = {}
        self.value_risk = {}
        self.risk_L = {}
        self.test = {}
        self.original_value_risk = {}
        print("[INFO] EnergyTrackingYOLO initialized")


    def __call__(self, *args, **kwargs):
        # 학습 모드일 때만 동작
        self.call_count += 1  # 호출 횟수 증가
        # Forward 메서드 실행
        return self.forward(*args, **kwargs)



#=======================스케일링 계수 설정========================================================



    def calculate_scaling_factors(self, original_data_dict, modified_data_dict):
        """
        원본 데이터가 수정된 데이터가 되기 위한 루트 관계 기반 스케일링 계수를 계산.

        Parameters:
            original_data_dict (dict): {layer_name: np.array} 형태의 원본 데이터 딕셔너리
            modified_data_dict (dict): {layer_name: np.array} 형태의 수정된 데이터 딕셔너리

        Returns:
            dict: {layer_name: list} 형태의 루트 관계를 반영한 스케일링 계수 딕셔너리
        """
        if original_data_dict.keys() != modified_data_dict.keys():
            raise ValueError("Original data and modified data must have the same keys.")

        scaling_factors_dict = {}
        print("계수스케일")
        for layer_name in original_data_dict.keys():
            original_data = original_data_dict[layer_name]
            modified_data = modified_data_dict[layer_name]


            risk_root = (modified_data/self.value_risk[layer_name])**0.5



            if original_data == 0:
                scaling_factors = 1  # 원본 데이터가 0이면 계수를 1로 처리
            else:
                scaling_factors = (2**(risk_root/original_data))**0.5 # 루트 관계 기반 스케일링 계산

            scaling_factors_dict[layer_name] = scaling_factors
        print(scaling_factors_dict)
        return scaling_factors_dict







#===================================층별 처리 과정===============================================================

    def process_data_by_layer(self, data_dict, data_dict_pos, tolerance=0.0001):
        """
        딕셔너리 형태로 층별 데이터를 처리하여 결과를 반환.

        Parameters:
            data_dict (dict): {layer_name: np.array} 형태의 입력 데이터
            target_dict (dict): {에너지 퍼센트: 목표 CV} 형태의 딕셔너리
            tolerance (float): 허용 오차

        Returns:
            dict: {layer_name: {"modified_data": np.array, "achieved_cvs": dict}} 형태의 결과
        """
        results = {}
        original_data = data_dict
        # layer_name = [layer_name for layer_name, layer_data in data_dict.items() if layer_data is not None]

        # layer_data = [layer_data for layer_name, layer_data in data_dict.items() if layer_data is not None]

        iteration = 0
        print("여기까지 됨")
        # 개별 층 데이터 처리
        # modified_data = self.adjust_data_for_ratio_range(data_dict, list(self.layer_risk_val_pos.keys()))
        d = data_dict_pos
        # 마지막 인덱스 제외
        print("# 마지막 인덱스 제외")

        # Calculate mean without including outliers
        filtered = {k: v for k, v in d.items() if abs(v - (mean := sum(d.values()) / len(d))) <= 0.22 * mean}
        # self.visualize_layer_risk(filtered)
        # Recalculate the mean with filtered data
        modified_data = {k: (mean := sum(filtered.values()) / len(filtered)) for k , _ in filtered.items()}
        # self.visualize_layer_risk(modified_data)
        print(modified_data)
        # 최종 결과 저장
        scaling_factors = self.calculate_scaling_factors(filtered, modified_data)
        print(len(modified_data))
        print(f"계수구함")
        results = {
            "modified_data": modified_data,
            "scaling_factors": scaling_factors,
        }

        return results


    # 안정적인 find_best_base 함수
    def find_best_base(self, norm):
      """
      주어진 비율에서 log_b (ratio) = b^2 의 양수 b 값을 찾습니다.
      """
      print("normL2 :", norm )
      def equation(b):
          return (abs(np.log(norm)) /abs(np.log(b))) - (b**2)
      if norm <= 0:
          return None
      result = minimize_scalar(lambda b: abs(equation(b)), bounds=(1e-5, 10000), method='bounded')
      if result.success and result.x > 0 :
          return result.x
      else:
          return None


    def apply_scaling_to_yolo_modules(self, scaling_factors):

        """
        YOLO 모델의 특정 모듈에 scaling_factors를 적용함.

        Parameters:
            yolo_model (torch.nn.Module): YOLO 모델 객체.
            scaling_factors (dict): {모듈 이름: 스케일링 계수} 형태의 딕셔너리.

        Returns:
            None: 모델의 파라미터가 수정됨.
        """
        yolo_model = self.model
        self.scaled_layer = []
        n = 0
        print(scaling_factors)
        for name, module in yolo_model.named_modules():  # YOLOv8 모델의 모듈 순회
            if name in scaling_factors and self.layer_risk_val_pos:  # 스케일링 계수가 정의된 모듈만 처리
                print(f"Processing module: {name}")
                scaling_factor = scaling_factors[name]
                print(f"Applying scaling to module: {name} with factor {scaling_factor}")
                for param_name, param in module.named_parameters(recurse=False):  # 모듈 내 파라미터 순회
                  if isinstance(module.weight, torch.Tensor) and not name.strip().endswith('.bn') and not name.strip().endswith('.bias'):
                    try:
                      print(f"  Processing parameter: {name}.{param_name}")
                      #가중치 파라미터에 적용
                      if param_name.strip().lower().endswith('weight'):
                        print(f"    Original Mean: {param.data.mean().item():.6f}, Std: {param.data.std().item():.6f}")
                        # Scaling factor 처리
                        # 한번 처리된거면 스킵
                        if name in self.scaled_layer:
                          continue
                        #파라미터 타입 확인

                        param *= scaling_factor
                        yolo_model.load_state_dict(yolo_model.state_dict())

                        self.scaled_layer.append(name)
                        n += 1

                        print(f"    Scaled Mean: {param.data.mean().item():.6f}, Std: {param.data.std().item():.6f}")
                    except Exception as e:
                        continue  # 에러가 발생해도 다른 파라미터 처리
        # self.calculate_layer_energies(True)
        print("처리된 개수 :",n)
        print("총 개수:", len(self.layer_energies.keys()))
        display(self.scaled_layer)
        display()
#============================================================기본적 계산=========================================================================================
    def calculate_layer_energies(self,cond = False):
        """
        레이어별 에너지 계산 및 로그 기반 통계 생성.
        """
        self.layer_risk_val_pos = {}
        self.energy_ratios = {}
        self.closest_log_bases = {}
        prev_energy = None
        layer_names = []
        layer_energy = 0
        value = {}
        val = {}
        branch = {}
        for name, module in self.model.named_modules():
          # 자식모듈이 없는 경우만
          if not any(module.children()):
            try:
              # 가중치 노름 계산
              weight_norm = torch.norm(module.weight, p=2).item()  # L2 노름
              print(weight_norm)
              layer_energy = (weight_norm ** 2)
              print(f"[INFO] Layer: {name}, Weight Norm: {layer_energy}")
              if layer_energy == 0:
                continue
            except Exception as e:
                print(f"[ERROR] Failed to process Layer: {name}, Error: {e}")
                continue


            branch[name] = self.find_best_base(layer_energy)
            self.risk_L[name] = branch[name]
            self.layer_energies[name] = layer_energy
            if hasattr(module, 'weight') and isinstance(module.weight, torch.Tensor) and not name.strip().endswith('.bn') and not name.strip().endswith('.bias') and (len(list(module.named_children())) == 0):
              self.layer_risk_val_pos[name] = 1

        # 모든 값을 정규화하여 대입
        total_energy = sum(self.layer_energies.values())
        value = {k: v / total_energy for k, v in self.layer_energies.items()}

        # 로그 변환 (값이 0인 경우 처리)
        value = {k: -(np.log2(v)) if v > 0 else float('-inf') for k, v in value.items()}
        self.value_risk = value

        # 결과 출력
        print(value)
        value_check = {}
        val = {key: (branch[key]) * value[key] for key in branch.keys() & value.keys()}
        for name , _ in val.items():
          if name in list(self.layer_risk_val_pos.keys()):
            self.layer_risk_val_pos[name] = val[name]
            value_check[name] = val[name]
        if cond:
          # self.visualize_layer_risk(value_check)
          # 함수 끝
          return None
        self.layer_risk_val = val
        print(self.layer_risk_val)

        if not any(self.original_value_risk.keys()):
          print("[INFO] original_value_risk is empty")
          self.original_value_risk = self.layer_risk_val_pos

        # 리스크가치 보상값
        data = self.layer_risk_val
        data_pos = self.layer_risk_val_pos
        # val 의 통계
        tolerance = 0.0001  # 허용 오차
        print(data)
        # 데이터 처리
        print("여기까지 됨")
        results = self.process_data_by_layer(data,data_pos, tolerance)
        print("여기까지 됨")
        scaling_factors = results["scaling_factors"]
        self.apply_scaling_to_yolo_modules(scaling_factors)

        return self.layer_energies, self.energy_ratios, self.closest_log_bases


    def visualize_layer_risk(self, layer_risk_val, max_layers_per_chart=200, label_skip=1):
        """
        시각화 함수: model.layer_risk_val 딕셔너리를 그래프로 시각화합니다.
        데이터 길이에 따라 자동으로 그래프 크기와 개수를 조정하며,
        x축 레이블을 적당히 생략하여 가독성을 높입니다.

        Parameters:
            layer_risk_val (dict): 레이어 이름을 키로, 위험 값을 값으로 가지는 딕셔너리.
            max_layers_per_chart (int): 한 그래프에 표시할 최대 레이어 수. 기본값은 10.
            label_skip (int): x축 레이블 표시 간격. 기본값은 1 (모든 레이블 표시).
        """

        if not layer_risk_val:
            print("No layer risk values to visualize.")
            return
        #모듈 인덱스 순으로 재정렬
        for key in sorted(layer_risk_val.keys()):
            layer_risk_val[key] = layer_risk_val.pop(key)

        # 레이어 이름과 값 분리
        layers = list(layer_risk_val.keys())
        risks = list(layer_risk_val.values())

        # 전체 데이터 분할
        total_layers = len(layers)
        num_charts = math.ceil(total_layers / max_layers_per_chart)  # 필요한 그래프 수 계산

        for i in range(num_charts):
            # 현재 차트에 표시할 데이터 슬라이스
            start_idx = i * max_layers_per_chart
            end_idx = start_idx + max_layers_per_chart
            current_layers = layers[start_idx:end_idx]
            current_risks = risks[start_idx:end_idx]

            # 레이블 생략 간격 계산
            skip_interval = max(1, math.ceil(len(current_layers) / (12 / label_skip)))
            adjusted_labels = [
                label if idx % skip_interval == 0 else ""
                for idx, label in enumerate(current_layers)
            ]

            # 그래프 그리기
            plt.figure(figsize=(min(len(current_layers) * 1.5, 12), 6))  # 크기 조정
            plt.bar(current_layers, current_risks, color='skyblue', edgecolor='black')
            plt.xticks(range(len(current_layers)), adjusted_labels, rotation=45, fontsize=10, ha='right')  # 레이블 조정
            plt.xlabel("Layers", fontsize=12)
            plt.ylabel("Risk Values", fontsize=12)
            plt.title(f"Layer Risk Values (Chart {i + 1}/{num_charts})", fontsize=14)
            plt.grid(axis='y', linestyle='--', alpha=0.7)
            plt.tight_layout()

            # 그래프 표시
            plt.show()



    @contextmanager
    def suppress_output(self):
        """Temporarily suppress all output to stdout."""
        with open(os.devnull, 'w') as devnull:
            old_stdout = sys.stdout
            try:
                sys.stdout = devnull
                yield
            finally:
                sys.stdout = old_stdout


    def forward(self, x):
      # with self.suppress_output():
        # 10번 호출될 때마다 추가 작업 실행
        if True:
          if True:
            try:
                print("[INFO] Forward started")
                self.calculate_layer_energies()
                self.model.load_state_dict(self.model.state_dict())
                print("[INFO] Energy calculations completed")
                self.call_count = 0
                return 0
            except Exception as e:
                print(f"\033[91m[ERROR] Error during forward pass: {e}\033[0m")
                raise
          else:
              return 0
        else:
          return 0

        return self.layer_risk_val




# 사용 예제
model = RISK_FULL('modified_conv_0_model_ratio_2 (1).pt')  # YOLO 모델 로드


input_tensor = torch.randint(0, 256, (1, 3, 640, 640), dtype=torch.float32)  # 0~255 정수 값 생성
input_tensor = input_tensor / 255.0  # 0~1로 정규화


try:
    for i in range(2):
      # model.forward(input_tensor)
      model.calculate_layer_energies()

      model.load_state_dict(model.state_dict())  # 업데이트된 가중치를 모델 상태에 동기화
    model.visualize_layer_risk(model.layer_risk_val_pos)
    model.visualize_layer_risk(model.original_value_risk)
    model.visualize_layer_risk(model.value_risk)
    model.visualize_layer_risk(model.risk_L)
except Exception as e:
    print(f"\033[91m[ERROR] Unexpected error: {e}\033[0m")

model.risk_L

model.calculate_layer_energies(True)

"""# 새 섹션"""

model.eval()
model.save()

import numpy as np
import matplotlib.pyplot as plt
from scipy.fftpack import fft


def analyze_band_energy_and_cv(data, sampling_rate, low_band=(0, 20), high_band=(20, 100)):
    """
    Analyze and visualize the energy and CV (coefficient of variation) for specified frequency bands.

    Args:
        data (np.array): Input time-domain signal data.
        sampling_rate (float): Sampling rate of the signal.
        low_band (tuple): Frequency range for the low band (min_freq, max_freq).
        high_band (tuple): Frequency range for the high band (min_freq, max_freq).

    Returns:
        None
    """
    # Perform Fourier Transform
    fft_result = fft(data)
    frequencies = np.fft.fftfreq(len(data), d=1/sampling_rate)
    power_spectrum = np.abs(fft_result)**2

    # Extract indices for low and high frequency bands
    low_freq_indices = np.where((frequencies >= low_band[0]) & (frequencies < low_band[1]))
    high_freq_indices = np.where((frequencies >= high_band[0]) & (frequencies < high_band[1]))

    # Compute energy for each band
    low_band_energy = power_spectrum[low_freq_indices]
    high_band_energy = power_spectrum[high_freq_indices]

    # Calculate CV for each band
    def calculate_cv(energy):
        mean_energy = np.mean(energy)
        std_energy = np.std(energy)
        return std_energy / mean_energy if mean_energy > 0 else 0

    cv_low = calculate_cv(low_band_energy)
    cv_high = calculate_cv(high_band_energy)

    # Visualization
    plt.figure(figsize=(12, 6))
    plt.bar(['Low Band', 'High Band'],
            [np.sum(low_band_energy), np.sum(high_band_energy)],
            color=['blue', 'orange'], alpha=0.7, label="Energy")
    plt.plot(['Low Band', 'High Band'],
             [cv_low * np.sum(low_band_energy), cv_high * np.sum(high_band_energy)],
             color='red', marker='o', linestyle='--', label="CV Weighted Energy")
    plt.title("Band Energy and CV Relationship", fontsize=16)
    plt.ylabel("Energy (Scaled)", fontsize=14)
    plt.legend(fontsize=12)
    plt.grid(alpha=0.3)
    plt.show()

    # Print CV values
    print(f"CV (Low Band {low_band[0]}-{low_band[1]} Hz): {cv_low:.2f}")
    print(f"CV (High Band {high_band[0]}-{high_band[1]} Hz): {cv_high:.2f}")

from statsmodels.tsa.seasonal import seasonal_decompose
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# STL 분해를 통한 시각화 (CV 포함)
def stl_decomposition_with_cv(data, period):
    mean = np.mean(data)
    decomposition = seasonal_decompose(data, model='additive', period=period)
    trend = decomposition.trend
    seasonal = mean + decomposition.seasonal
    residual = mean + decomposition.resid

    # CV 계산 함수
    def calculate_cv(series):
      return np.std(series) / np.mean(series)

    # CV 값 계산
    cv_original = calculate_cv(data)
    cv_trend = calculate_cv(trend)
    cv_seasonal = calculate_cv(seasonal)
    cv_residual = calculate_cv(residual)

    # 시각화
    plt.figure(figsize=(12, 10))
    plt.subplot(4, 1, 1)
    plt.plot(data, label=f'Original Data (CV={cv_original:.2f})')
    plt.title('Original Data')
    plt.legend()

    plt.subplot(4, 1, 2)
    plt.plot(trend, label=f'Trend (CV={cv_trend:.2f})')
    plt.title('Trend')
    plt.legend()

    plt.subplot(4, 1, 3)
    plt.plot(seasonal, label=f'Seasonality (CV={cv_seasonal:.2f})')
    plt.title('Seasonality')
    plt.legend()

    plt.subplot(4, 1, 4)
    plt.plot(residual, label=f'Residual (CV={cv_residual:.2f})')
    plt.title('Residual')
    plt.legend()

    plt.tight_layout()
    plt.show()

    return trend, seasonal, residual

# STL 분해
trend, seasonal, residual = stl_decomposition_with_cv(list(model.layer_risk_val.values()), period=12)

import numpy as np
import matplotlib.pyplot as plt

import numpy as np
import matplotlib.pyplot as plt

def plot_derivatives_from_data(data):
    """
    Plots the original data, first derivative, second derivative, and their ratio (second/first derivative).

    Args:
        data (list or numpy array): Input time series data.
    """
    # Ensure data is a numpy array
    data = np.array(data)

    # Compute first derivative (forward difference)
    first_derivative = np.diff(data)

    # Compute second derivative (difference of first derivative)
    second_derivative = np.diff(first_derivative)

    # Align lengths by trimming the first value from the first derivative
    first_derivative_trimmed = first_derivative[:-1]  # Match length with second_derivative

    # Compute ratio (second derivative / first derivative)
    ratio = np.abs(data[1:] * first_derivative)
    np.multiply(ratio, 100)
    # Create x-axis for plotting
    x_original = np.arange(len(data))
    x_first = np.arange(len(first_derivative))
    x_second = np.arange(len(second_derivative))

    # Plot the graphs
    plt.figure(figsize=(12, 10))

    # Plot original data
    plt.plot(x_original, data, label="Original Data", linewidth=2)

    # Plot first derivative
    plt.plot(x_first, first_derivative, label="First Derivative (Speed)", linestyle='--', linewidth=2)

    # Plot second derivative
    plt.plot(x_second, second_derivative, label="Second Derivative (Acceleration)", linestyle=':', linewidth=2)

    # Plot ratio (second/first derivative)
    x_ratio = np.arange(len(ratio))
    plt.plot(x_ratio, ratio, label="Acceleration / Speed Ratio", linestyle='-.', linewidth=2)

    # Add titles and labels
    plt.title("Original Data and Derivatives with Ratio", fontsize=16)
    plt.xlabel("Index", fontsize=14)
    plt.ylabel("Value / Rate of Change", fontsize=14)
    plt.axhline(0, color='black', linewidth=0.8, linestyle='--')
    plt.legend(fontsize=12)
    plt.grid(alpha=0.3)

    # Show the plot
    plt.show()
    return


# Call the function
ratio = plot_derivatives_from_data(list(model.layer_risk_val.values()))
display(ratio)

input_tensor = torch.randint(0, 256, (1, 3, 640, 640), dtype=torch.float32)  # 0~255 정수 값 생성
input_tensor = input_tensor / 255.0  # 0~1로 정규화
for i in range(10):
  model.calculate_layer_energies()

def plot_efficient(expected_returns, risks, risk_free_rate=0.02):
    """
    Plot Efficient Frontier, Capital Allocation Line (CAL), and an ideal curve for comparison.

    Args:
    - expected_returns (dict): Dictionary of asset names and expected returns.
    - risks (dict): Dictionary of asset names and standard deviations (risks).
    - risk_free_rate (float): The risk-free rate of return.

    Returns:
    - None
    """
    # 데이터 교집합 필터링
    common_assets = set(expected_returns.keys()) & set(risks.keys())
    if not common_assets:
        print("[ERROR] No common assets found.")
        return

    # 교집합 데이터 추출
    filtered_expected_returns = {k: expected_returns[k] for k in common_assets}
    filtered_risks = {k: risks[k] for k in common_assets}

    # 기대수익률과 리스크 데이터
    assets = list(filtered_expected_returns.keys())
    returns = np.array([filtered_expected_returns[asset] for asset in assets])
    std_devs = np.array([filtered_risks[asset] for asset in assets])

    # Tangency Portfolio (최대 샤프 비율) 계산
    sharpe_ratios = (returns - risk_free_rate) / std_devs
    max_sharpe_idx = np.argmax(sharpe_ratios)
    tangency_return = returns[max_sharpe_idx]
    tangency_risk = std_devs[max_sharpe_idx]

    # 이상적 곡선 데이터 생성 (예: sqrt(x) 또는 x^0.5)
    ideal_risks = np.linspace(0, max(std_devs) * 1.2, 100)
    ideal_returns = np.sqrt(ideal_risks) * (max(returns) / np.sqrt(max(ideal_risks)))

    # 그래프 설정
    plt.figure(figsize=(12, 8))
    plt.scatter(std_devs, returns, c="gold", label="Individual Assets", edgecolors="black", zorder=5, s=50)

    # Efficient Frontier
    sorted_indices = np.argsort(std_devs)
    sorted_std_devs = std_devs[sorted_indices]
    sorted_returns = returns[sorted_indices]
    plt.plot(
        sorted_std_devs,
        sorted_returns,
        label="Efficient Frontier",
        color="green",
        linestyle="-",
        zorder=4
    )

    # Capital Allocation Line (CAL)
    cal_x = np.linspace(0, max(std_devs) * 1.2, 100)
    cal_y = risk_free_rate + (tangency_return - risk_free_rate) / tangency_risk * cal_x
    plt.plot(cal_x, cal_y, label="Capital Allocation Line (CAL)", color="blue", linestyle="--", zorder=3)

    # Tangency Portfolio 표시
    plt.scatter([tangency_risk], [tangency_return], color="red", label="Tangency Portfolio", zorder=6, s=100)

    # Risk-Free Rate 선 표시
    plt.axhline(y=risk_free_rate, color='gray', linestyle='--', label="Risk-Free Rate", zorder=2)

    # 이상적 곡선 추가
    plt.plot(
        ideal_risks,
        ideal_returns,
        label="Ideal Curve (e.g., sqrt(x))",
        color="purple",
        linestyle=":",
        linewidth=2,
        zorder=1
    )

    # 축 범위 조정
    plt.xlim(0, max(std_devs) * 1.1)
    plt.ylim(0, max(returns) * 1.1)

    # 축 레이블, 제목, 범례
    plt.title("Efficient Frontier, CAL, and Ideal Curve", fontsize=14)
    plt.xlabel("Standard Deviation (Risk)", fontsize=12)
    plt.ylabel("Expected Return", fontsize=12)
    plt.legend(fontsize=10)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.show()

import torch
from ultralytics import YOLO

# YOLO 모델 로드
model = YOLO('modified_conv_0_model_ratio_2.pt')

def print_layer_tree(module, prefix=""):
    """
    재귀적으로 모델 계층을 트리 형식으로 출력하는 함수
    """
    n = 0
    for name, child in module.named_children():
        print(f"{prefix}├── {name}: {type(child).__name__}")
        print_layer_tree(child, prefix + "│   ")
        n += 1
    return n

print("Model Layer Tree:")
print_layer_tree(model.model)

import torch
from ultralytics import YOLO

# YOLO 모델 로드
model = YOLO('yolov8n')

def print_layer_tree(module, prefix=""):
    """
    재귀적으로 모델 계층을 트리 형식으로 출력하는 함수
    """
    n = 0
    for name, child in module.named_children():
        print(f"{prefix}├── {name}: {type(child).__name__}")
        print_layer_tree(child, prefix + "│   ")
        n += 1
    return n

print("Model Layer Tree:")
print_layer_tree(model.model)

from google.colab import drive
drive.mount('/content/drive')

# Images directory
image_dir = "/content/drive/MyDrive/종합설계/version/최종데이터/data.yaml"

# Train the modified model
model.train(data=image_dir, epochs=50, imgsz=640)

model.save()

from ultralytics import YOLO
YOLO('yolov8n.pt').train(data='coco128.yaml', epochs=100, imgsz=640, batch=16, device=0)
model.save()

"""테스트"""

from ultralytics import YOLO
import matplotlib.pyplot as plt
import cv2

# # 학습된 모델 로드
# model = YOLO('/content/drive/MyDrive/project/dataset_3/last_1.pt')  # 'best.pt'는 학습된 가중치 파일 경로

# 이미지에서 객체 감지 실행
results = model.predict(source='test.jpg', show=True, conf=0.25)

image_with_boxes = results[0].plot()

# Matplotlib로 시각화
plt.figure(figsize=(10, 10))
plt.imshow(cv2.cvtColor(image_with_boxes, cv2.COLOR_BGR2RGB))
plt.axis('off')  # 축 제거
plt.show()

# Load original YOLOv8 model for comparison
original_model = YOLO("best.pt")
model = YOLO('/content/runs/detect/train/weights/last.pt')
# model = RISK_FULL("best.pt")
image_dir_test = "/content/drive/MyDrive/종합설계/데이터셋/data.yaml"
# coco dataset
image_dir_test


# # Test original model
# original_results = original_model.predict(source=image_dir_test, save=True)

# Compare mAP
# 원본 모델 평가
original_metrics = original_model.val(data=image_dir_test)  # 'coco128.yaml'은 데이터셋 설정 파일로, 필요한 경우 다른 경로로 변경하세요.


# 수정된 모델 평가
modified_metrics = model.val(data=image_dir_test)  # 'coco128.yaml'은 데이터셋 설정 파일로, 필요한 경우 다른 경로로 변경하세요.

"""##coco 여러모델 비교"""

# 여러 모델 평가
models = ["yolov8n"]
results_list = []

for model_name in models:

    model = RISK_FULL(model_name)
    input_tensor = torch.randint(0, 256, (1, 3, 640, 640), dtype=torch.float32)  # 0~255 정수 값 생성
    input_tensor = input_tensor / 255.0  # 0~1로 정규화
    model.forward(input_tensor)
    results = model.val(data="coco128.yaml", conf=0.5, verbose=False)
    print(results.fitness)
    # DetMetrics 객체에서 핵심 지표 추출
    metrics = {
        "Model": model_name,
        "Precision": np.mean(results.box.p),  # Precision
        "Recall": np.mean(results.box.r),     # Recall
        "mAP@50": results.box.map50, # mAP@50
        "mAP@50-95": results.box.map # mAP@50-95
    }
    results_list.append(metrics)
        # 모델 로드 및 평가
    model = YOLO(model_name)
    results = model.val(data="coco128.yaml", conf=0.5, verbose=False)

    # DetMetrics 객체에서 핵심 지표 추출
    metrics = {
        "Model": model_name,
        "Precision": np.mean(results.box.p),  # Precision
        "Recall": np.mean(results.box.r),     # Recall
        "mAP@50": results.box.map50, # mAP@50
        "mAP@50-95": results.box.map # mAP@50-95
    }
    results_list.append(metrics)

# DataFrame으로 정리
df = pd.DataFrame(results_list)
display(df)
print(modeled.call_count)

modeled.

model.visualize_layer_risk(model.layer_risk_val_pos)
model.visualize_layer_risk(model.original_value_risk)

# 여러 모델 평가
models = ["modified_conv_0_model_ratio_2 (1).pt"]
results_list = []

for model_name in models:

    model = RISK_FULL(model_name)
    input_tensor = torch.randint(0, 256, (1, 3, 640, 640), dtype=torch.float32)  # 0~255 정수 값 생성
    input_tensor = input_tensor / 255.0  # 0~1로 정규화
    model.forward(input_tensor)
    results = model.val(data=image_dir, conf=0.4, verbose=False)
    result1 = results
    # DetMetrics 객체에서 핵심 지표 추출
    metrics = {
        "Model": model_name,
        "Precision": np.mean(results.box.p),  # Precision
        "Recall": np.mean(results.box.r),     # Recall
        "mAP@50": results.box.map50, # mAP@50
        "mAP@50-95": results.box.map, # mAP@50-95
        "fitness" : results.fitness
    }
    results_list.append(metrics)

    # 모델 로드 및 평가
    model = YOLO(model_name)
    results = model.val(data=image_dir, conf=0.389, verbose=False)
    result2 = results
    # DetMetrics 객체에서 핵심 지표 추출
    metrics = {
        "Model": model_name,
        "Precision": np.mean(results.box.p),  # Precision
        "Recall": np.mean(results.box.r),     # Recall
        "mAP@50": results.box.map50, # mAP@50
        "mAP@50-95": results.box.map, # mAP@50-95
        "fitness" : results.fitness
    }
    results_list.append(metrics)

# DataFrame으로 정리
df = pd.DataFrame(results_list)
display(df)

print(result1)
print(result2)

# 여러 모델 평가
models = ["yolov8n", "yolov8x", "yolov11n", "yolov11x"]  # 사용할 모델들 추가
results_list = []

for model_name in models:
    # 모델 로드 및 평가
    model = YOLO(model_name)

    # 이미지 데이터 경로 및 설정
    results = model.val(data="coco128.yaml", conf=0.5, verbose=False)

    # 기본 성능 지표 추출
    metrics = {
        "Model": model_name,
        "Precision": np.mean(results.box.p),   # Precision
        "Recall": np.mean(results.box.r),      # Recall
        "mAP@50": results.box.map50,          # mAP@50
        "mAP@50-95": results.box.map          # mAP@50-95
    }

    # 추가로 IoU 별 성능 (예: mAP@75)
    metrics["mAP@75"] = results.box.map75  # mAP@75 직접 접근

    # 클래스별 Precision, Recall, mAP 추출
    num_classes = len(results.box.p)  # 클래스 개수
    for cls_id in range(num_classes):
        metrics[f"Class_{cls_id}_Precision"] = results.box.p[cls_id]
        metrics[f"Class_{cls_id}_Recall"] = results.box.r[cls_id]
        metrics[f"Class_{cls_id}_mAP"] = results.box.maps()[cls_id]

    # False Positive Rate (FPR) & True Negative Rate (TNR) 계산
    metrics["False Positive Rate"] = np.mean(results.box.fp)
    metrics["True Negative Rate"] = np.mean(results.box.tn)

    results_list.append(metrics)

    # RISK_FULL 모델 평가 추가
    model = RISK_FULL(model_name)
    input_tensor = torch.randint(0, 256, (1, 3, 640, 640), dtype=torch.float32)  # 0~255 정수 값 생성
    input_tensor = input_tensor / 255.0  # 0~1로 정규화
    model.forward(input_tensor)
    results = model.val(data="coco128.yaml", conf=0.5, verbose=False)

    # RISK_FULL에서 평가 지표 추가
    risk_metrics = {
        "Model": f"{model_name}_RISK_FULL",
        "Precision": np.mean(results.box.p),
        "Recall": np.mean(results.box.r),
        "mAP@50": results.box.map50,
        "mAP@50-95": results.box.map,
        "Latency": results.latency,  # Latency 평가 (예: FPS, 처리 시간)
    }

    results_list.append(risk_metrics)

# DataFrame으로 정리
df = pd.DataFrame(results_list)

# 지표 출력
display(df)

import yaml

# 수정할 yaml 파일 경로
yaml_path = '/content/drive/MyDrive/종합설계/version/최종데이터/data.yaml'

# 새 경로 지정
new_train_path = '/content/drive/MyDrive/종합설계/version/최종데이터/train/images'
new_val_path = '/content/drive/MyDrive/종합설계/version/최종데이터/val/images'

# YAML 파일 읽기
with open(yaml_path, 'r') as file:
    data = yaml.safe_load(file)

# 경로 수정
data['train'] = new_train_path
data['val'] = new_val_path

# YAML 파일 저장
with open(yaml_path, 'w') as file:
    yaml.dump(data, file)

print(f"Updated paths in {yaml_path} successfully!")